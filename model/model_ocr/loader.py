from ocr_utils import download_youtube_video, extract_frames_from_video, preprocess_image_for_ocr
from yolov5.utils.general import non_max_suppression, scale_boxes
from yolov5.utils.augmentations import letterbox
from yolov5.models.common import DetectMultiBackend
from yolov5.utils.torch_utils import select_device
import os
import torch
import re
import numpy as np
import cv2
import logging
import requests
import uuid
import time
import json
import tempfile
from typing import List, Dict, Any, Tuple, Optional
from dotenv import load_dotenv 
'''
ì°¸ê³ 
model_path = 
conf_threshold = 0.3 
'''

load_dotenv()
logger = logging.getLogger("subtitle-detector.loader")

class YOLOSubtitleDetector:
    """
    YOLO ëª¨ë¸ì„ ì‚¬ìš©í•œ ìë§‰ ê²€ì¶œ ë° OCR ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, model_path: str, ocr_secret_key: str, ocr_invoke_url: str, conf_threshold: float = 0.1):
        """
        ì´ˆê¸°í™” í•¨ìˆ˜
        
        Args:
            model_path (str): YOLO ëª¨ë¸ ê²½ë¡œ "/model_ocr/yolo_best.pt"
            ocr_secret_key (str): Clova OCR API í‚¤
            ocr_invoke_url (str): Clova OCR API URL
            conf_threshold (float): ê²€ì¶œ ì‹ ë¢°ë„ ì„ê³„ê°’ìœ¼ë¡œ ì¤„ì´ë©´ ë” ë§ì´ ì¡ëŠ”ë° ì˜¤íƒì´ ë§ì•„ì§ í˜„ì¬ 0.3
        """
        self.model_path = "gaboljido_yolo.torchscript"
        self.ocr_secret_key = os.getenv("CLOVA_OCR_SECRET_KEY") or ocr_secret_key
        self.ocr_invoke_url = os.getenv("CLOVA_OCR_API_URL") or ocr_invoke_url
        self.conf_threshold = conf_threshold
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # âœ… device ì €ì¥
        self.model = self._load_model()

    def _load_model(self):
        """YOLO ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"YOLO ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # TorchScript ëª¨ë¸ ë¡œë“œ
            model = DetectMultiBackend(self.model_path, device=self.device)
            model.eval()

            logger.info(f"{self.device.type.upper()}ì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return model

        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
    def detect_subtitle_crops_single(self, images: List[np.ndarray]) -> List[List[np.ndarray]]:
        all_crops = []

        for idx, image in enumerate(images):
            try:
                # ì „ì²˜ë¦¬
                if image.ndim == 2:
                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
                elif image.shape[2] == 4:
                    image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)
                else:
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

                orig_h, orig_w = image.shape[:2]
                resized = cv2.resize(image, (640, 640))
                tensor = torch.from_numpy(resized).permute(2, 0, 1).float().unsqueeze(0) / 255.0
                tensor = tensor.to(self.device)

                # ì¶”ë¡ 
                with torch.no_grad():
                    preds = self.model(tensor)
                    if isinstance(preds, (list, tuple)):
                        preds = preds[0]
                    if preds.ndim == 2:
                        preds = preds.unsqueeze(0)

                # NMS
                dets = non_max_suppression(preds, conf_thres=self.conf_threshold, iou_thres=0.45)[0]

                # ë°•ìŠ¤ ë³µì› ë° í¬ë¡­
                crops = []
                if dets is not None and len(dets):
                    for *xyxy, conf, cls in dets:
                        x1, y1, x2, y2 = [int(v.item()) for v in xyxy]

                        # ì›ë³¸ í•´ìƒë„ì— ë§ê²Œ ìŠ¤ì¼€ì¼ ì¡°ì •
                        scale_x = orig_w / 640
                        scale_y = orig_h / 640
                        x1 = int(x1 * scale_x)
                        y1 = int(y1 * scale_y)
                        x2 = int(x2 * scale_x)
                        y2 = int(y2 * scale_y)

                        crop = image[y1:y2, x1:x2]
                        crops.append(crop)

                print(f"[DEBUG] Frame {idx+1}/{len(images)}: ê²€ì¶œëœ í¬ë¡­ ìˆ˜ = {len(crops)}")
                all_crops.append(crops)

            except Exception as e:
                print(f"[ERROR] Frame {idx+1}: {e}")
                all_crops.append([])

        return all_crops

        
    def _call_clova_ocr_image(self, image_np_array, ocr_invoke_url, ocr_secret_key):
        """
        YOLO ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ìë§‰ ì˜ì—­ ê²€ì¶œ í›„, í•´ë‹¹ ì˜ì—­ì˜ í¬ë¡­ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜

        Args:
            image_np_array : detect_subtitle_cropsì—ì„œ returní•œ cropëœ ì´ë¯¸ì§€ë“¤ë“¤
            api_url : clova api_url
            secret_key : clova secret key

        Returns:
            
        """
        # numpy arrayë¥¼ JPEG ë°”ì´íŠ¸ë¡œ ë³€í™˜
        _, img_encoded = cv2.imencode('.jpg', image_np_array)
        image_bytes = img_encoded.tobytes()

        request_json = {
            'images': [
                {
                    'format': 'jpg',
                    'name': 'box_crop'
                }
            ],
            'requestId': str(uuid.uuid4()),
            'version': 'V2',
            'timestamp': int(round(time.time() * 1000))
        }

        payload = {'message': json.dumps(request_json).encode('UTF-8')}
        files = [('file', ('crop.jpg', image_bytes, 'image/jpeg'))]
        headers = {
            'X-OCR-SECRET': ocr_secret_key
        }

        try:
            response = requests.post(ocr_invoke_url, headers=headers, data=payload, files=files, timeout=10)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ CLOVA OCR ìš”ì²­ ì‹¤íŒ¨: {e}")
            return {"images": [{"fields": []}]}  # ê¸°ë³¸ ë¹ˆ ì‘ë‹µ
    
    
    def _extract_text_from_ocr_result(self, result: Dict[str, Any]) -> str:
        """
        OCR ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            result (Dict[str, Any]): OCR ê²°ê³¼ JSON
            
        Returns:
            str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        try:
            texts = []
            
            for field in result.get('images', [])[0].get('fields', []):
                text = field.get('inferText', '').strip()
                if text:
                    texts.append(text)
            
            return " ".join(texts)
        
        except Exception as e:
            logger.error(f"OCR ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    def extract_text_with_ocr(self, crops: List[np.ndarray]) -> List[str]:
        """
        ì—¬ëŸ¬ ê°œì˜ í¬ë¡­ëœ ì´ë¯¸ì§€ì— ëŒ€í•´ OCR ìˆ˜í–‰í–‰

        Args:
            crops : numpy ë°°ì—´ì˜ cropëœ ì´ë¯¸ì§€ë“¤ë“¤

        Returns:
            List[str]: ê° ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        texts = []
        crops = [crop for batch in crops for crop in batch]
        for crop in crops:
            if crop.size == 0 or crop.shape[0] < 10 or crop.shape[1] < 10:
                texts.append("")
                continue
            # ì „ì²˜ë¦¬
            preprocessed = preprocess_image_for_ocr(crop)

            # í´ë¡œë°” OCR ë°”ë¡œ í˜¸ì¶œ (NumPy ë°°ì—´ ì‚¬ìš©) call_clova_ocr_imageëŠ” ì¸ì‹ê¹Œì§€ë§Œ í•˜ê³  _extract_text_from_ocr_resultëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œì¶œ
            ocr_result = self._call_clova_ocr_image(preprocessed, self.ocr_invoke_url, self.ocr_secret_key)
            text = self._extract_text_from_ocr_result(ocr_result)
            texts.append(text)

        return texts
    
    def process_youtube_pipeline(self, youtube_url: str, interval_sec: float = 1.5, region_code: Optional[int] = None) -> List[str]:

        # 1. ìœ íŠœë¸Œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ
        video_data = download_youtube_video(youtube_url)
        if video_data is None:
            logger.error("âŒ YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            return []
        print("youtube download ì™„ë£Œë£Œ")

        # 2. í”„ë ˆì„ ì¶”ì¶œ
        frames_info = extract_frames_from_video(video_data["frames"], interval_sec=1.5)
        print(f"ğŸ–¼ï¸ ì´ {len(frames_info)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ")

        # 3. ìë§‰ ê²€ì¶œ + OCR
        raw_texts = []
        for i, frame_info in enumerate(frames_info):
            image = frame_info.get("image")
            if image is None:
                continue

            crops = self.detect_subtitle_crops_single([image])
            print(f"[DEBUG] Frame {i + 1}/{len(frames_info)}: ê²€ì¶œëœ í¬ë¡­ ìˆ˜ = {len(crops)}")
            if len(crops) == 0:
                continue

            texts = self.extract_text_with_ocr(crops)
            for crop_idx, text in enumerate(texts):
                if not text.strip():
                    continue
                logger.info(f"[Frame {i}] Crop {crop_idx}: '{text.strip()}'")
                raw_texts.append(text.strip())
        print("ocr ì¶”ì¶œ ì™„ë£Œë£Œ")

        # âœ… ì¤‘ë³µ ì œê±°
        deduped_texts = []
        if raw_texts:
            seen = set()
            for text in raw_texts:
                # í•œê¸€ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
                korean_only = re.sub(r'[^ê°€-í£\s]', '', text)
                korean_only = korean_only.strip()
                if korean_only and korean_only not in seen:
                    seen.add(korean_only)
                    deduped_texts.append(korean_only)

        #print(f"ìµœì¢… ë°˜í™˜ text:{deduped_texts}")
        logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {len(deduped_texts)}ê°œ ìë§‰ ê²€ì¶œë¨")
        return deduped_texts


    
    def __del__(self):
        """ì†Œë©¸ì: ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ"""
        if hasattr(self, 'model') and self.model is not None:
            del self.model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
